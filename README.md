# pg-to-snowflake-streaming

ğŸš€ A Real-Time Data Pipeline for streaming changes from PostgreSQL to Snowflake using Kafka & Debezium.

## ğŸ“Œ Overview

This project demonstrates a real-time Change Data Capture (CDC) pipeline built as part of my Data Engineering journey at the **Information Technology Institute (ITI)**.

The pipeline captures live changes from a PostgreSQL database and streams them to Snowflake using Kafka Connect, enabling near real-time analytics.

---

## âš™ï¸ Tech Stack

- **PostgreSQL** â€“ Source database
- **Debezium** â€“ Captures CDC (Change Data Capture) events from PostgreSQL
- **Apache Kafka** â€“ Message broker to stream the events
- **Kafka Connect Snowflake Sink Connector** â€“ Loads streaming data into Snowflake
- **Snowflake** â€“ Cloud data warehouse for storing and analyzing the streamed data
- **Docker & Docker Compose** â€“ For containerized and reproducible setup
- **Public/Private Key Authentication** â€“ For secure Snowflake access

---

## ğŸ”„ How It Works

1. **Debezium** monitors the PostgreSQL database for insert/update events.
2. The captured changes are published to **Kafka topics**.
3. **Kafka Connect** picks up these messages and sends them to **Snowflake** via the Snowflake Sink Connector.
4. **Data lands in Snowflake** in near real-time, ready for querying and analysis.

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ docker-compose.yml        # Defines all services (Postgres, Kafka, Connect, etc.)
â”œâ”€â”€ debezium-config           # Configs for Debezium connector
â”œâ”€â”€ snowflake-connector-config # JSON config for Snowflake Sink
â”œâ”€â”€ init.sql                  # Sample data + table creation
â””â”€â”€ README.md                 # You're here!
